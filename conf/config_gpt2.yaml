model:
  dim: 768
  num_heads: 12
  bias: False
  num_blocks: 12
  context_length: 1024
  rescale_residuals: False 


train:
  # dataset. supposrts "c4", "pile"
  dataset: "pile"

  max_steps: 50000

  # clip the gradient to have l2 norm at most this value
  gradient_clip_val: 10.0

  # optimizer/online learner name.
  # optimizers include "adamw", "sgdm", "polar", "jump"
  # online learners include "ogd_md"
  optimizer: "adamw"

  # whether to wrap the optimizer with online to nonconvex conversion
  # for some most optimizers/online learners, they have default value of wrap_o2nc 
  # (e.g., some online learners are always wrapped, and some optimizers are never wrapped),
  # which overwrites this setting.
  wrap_o2nc: False

  # random scaling options. supports "exponential", "None"
  random_scaling: "exponential"
  random_scaling_seed: 0

  # whether to use automatic mixed precision
  use_amp: True
  # value to cast to in mixed precision training.
  precision: float16
  
  batch_size: 4 # number of examples placed on each GPU

  # shuffle buffer for data loader. 0 means to shuffle.
  shuffle_buffer_size: 0

  # wandb_project: "gpt2_c4(conclusive_report)"
  wandb_project: "gpt2_pile"
  wandb_logs_per_sec: 1.0
  
  # this will slow down computation a bit (I believe due to extra GPU/CPU communication),
  # but will log more stuff (like learning rates).
  # Still working on nice way to do this logging - we really should only incur one communication
  # round per iteration and I don't think the logging data should significantly impact it.
  log_callback_data: True

  # logging these additional stats requires another forward pass and additional memory.
  # you can save memory / speed by turn off each of the following configurations
  # TODO: update the log_config to be more specific
  log_config:
    # In terms of computation memory:
    # stores the last gradient g(n-1)
    store_last_grads: True
    # stores the sum of past gradients g(1:n-1)
    store_past_grads: True
    # stores the change in parameter x(n) - x(n-1)
    store_last_params: True
    # In terms of computation speed:
    # computes f(x(n-1), zn), which costs an additional forward pass
    compute_last_loss: True
    # computes g(x(n-1), zn), which costs an additional forward and backward pass
    compute_last_grads: False
    # TODO: to be deprecated
    grads: True
    loss: True
  
  running_stats_window: 1000

  # following settings chosen after
  # some experimentation with a tiny model.
  # may not be optimal for all machines, but
  # hopefully with a reasonably sized model this will
  # prevent dataloading from being the bottleneck.
  dataloader_workers: 2


checkpoint:
  # checkpoint mode if path is not null
  path: null
  num_steps: 50000
  save_steps: 10000
  load_model: null
  # USE WITH CAUTION: if true, overwrites the entire config file (besides checkpoint)
  overwrite: False


# ========================================================================
# Below are specific configurations for each optimzer / online learner.
# Most configurations are tuned to be optimal.
# ========================================================================
adamw:
  lr_config:
    lr: 3e-4
    schedule: "linear"
    warmup: 5000
    max_steps: 50000
  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.1
  debias_beta1: True
  debias_beta2: True


sgdm:
  lr_config:
    lr: 3.0
    schedule: "linear"
    warmup: 5000
    max_steps: 50000
  beta: 0.99
  weight_decay: 1e-6


polar:
  direction:
    optim: "adamw"
    lr_config:
      lr: 3e-4
      schedule: "linear"
      warmup: 5000
      max_steps: 50000
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    debias_beta1: True
    debias_beta2: True

  magnitude:
    optim: "adamw"
    lr_config:
      lr: 1e-6
      schedule: "linear"
      warmup: 5000
      max_steps: 50000
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    debias_beta1: True
    debias_beta2: True


jump:
  normal_steps: 4500
  jump_steps: 500

  normal:
    optim: "adamw"
    lr_config:
      lr: 3e-4
      schedule: "linear"
      warmup: 450
      max_steps: 4500
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.1
    debias_beta1: True
    debias_beta2: True

  jump:
    optim: "adamw"
    lr_config:
      lr: 1e-6
      schedule: "linear"
      warmup: 50
      max_steps: 500
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.0
    debias_beta1: True
    debias_beta2: True


ogd_md:
  lr_config:
    lr: 0.01
    schedule: "linear"
    warmup: 5000
  beta: 0.99
  mu: 0.0


blackbox:
  magnitude: "kt"
  direction: "ftrl"