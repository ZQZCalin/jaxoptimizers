model:
  dim: 768
  num_heads: 12
  bias: False
  num_blocks: 12
  context_length: 1024
  rescale_residuals: False 

train:
  # dataset: "c4"
  dataset: "pile"
  # Note: currently this data_path is NOT used in train.py
  data_path: "/projectnb/aclab/datasets/c4/en/"

  max_steps: 50000

  # clip the gradient to have l2 norm at most this value
  gradient_clip_val: 10.0

  # random scaling options
  random_scaling: "exponential"
  random_scaling_seed: 0

  # whether to use automatic mixed precision
  use_amp: True
  # value to cast to in mixed precision training.
  precision: float16
  
  batch_size: 4 # number of examples placed on each GPU

  # wandb_project: "gpt2_c4(conclusive_report)"
  wandb_project: "gpt2_pile"
  wandb_logs_per_sec: 1.0

  # if true, runs benchmark optimizers. otherwise, defaults to 
  # run online-to-nonconvex algorithms with config.optimizer.
  run_benchmark: False
  
  # this will slow down computation a bit (I believe due to extra GPU/CPU communication),
  # but will log more stuff (like learning rates).
  # Still working on nice way to do this logging - we really should only incur one communication
  # round per iteration and I don't think the logging data should significantly impact it.
  log_callback_data: True

  # logging these additional stats requires another forward pass and additional memory.
  # you can save memory / speed by turn off each of the following configurations
  # TODO: update the log_config to be more specific
  log_config:
    # In terms of computation memory:
    # stores the last gradient g(n-1)
    last_grads: True
    # stores the sum of past gradients g(1:n-1)
    past_grads: True
    # stores the change in parameter x(n) - x(n-1)
    last_params: True
    # In terms of computation speed:
    # computes f(x(n-1), zn), which costs an additional forward pass
    compute_forward: True
    # computes g(x(n-1), zn), which costs an additional forward and backward pass
    compute_backward: False
    # TODO: to be deprecated
    grads: True
    loss: True
  
  running_stats_window: 1000

  # following settings chosen after
  # some experimentation with a tiny model.
  # may not be optimal for all machines, but
  # hopefully with a reasonably sized model this will
  # prevent dataloading from being the bottleneck.
  dataloader_workers: 2

optimizer:
  # base online learner.
  # ["ada_ftrl", "normalized_blackbox", "kt_blackbox", "ogd_mirror_descent"]
  online_learner: "ogd_mirror_descent"

  # learning rate scheduler
  lr_config:
    lr: 0.01
    schedule: "cosine"
    warmup: 5000

  # configuration of the base online learner.
  # for simplicity of management, each configuration is separated.
  ada_ftrl_config:
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    scale_exponential: False 
    scale_lr: False 
    scale_eps: False
    
  kt_blackbox_config:
    eps: 5
    Lipschitz: 20
    beta: 1.0
    weight_decay: 0.0
    per_layer: True

  ogd_md_config:
    beta: 0.99
    mu: 0.0


benchmark:
  # ["adamw", "sgdm", "polar", "jump"]
  optim: "adamw"

  lr_config:
    lr: 3e-4
    schedule: "cosine"
    warmup: 5000

  adamw_config:
    beta1: 0.9
    beta2: 0.999
    weight_decay: 0.01
    debias_beta1: True
    debias_beta2: True

  sgdm_config:
    beta: 0.99
    weight_decay: 0.0

  polar:
    b1: 0.9
    b2: 0.999
    eps: 1e-8
    direction_wd: 0.1
    magnitude_wd: 0.1

  jump:
    normal_steps: 4500
    jump_steps: 500
    lr1: 3e-4
    lr2: 1e-6
    wd1: 0.1
    wd2: 0.0