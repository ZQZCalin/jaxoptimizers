model:
  dim: 768
  num_heads: 12
  bias: False
  num_blocks: 12
  context_length: 1024
  rescale_residuals: False 

train:
  data_path: "/projectnb/aclab/datasets/c4/en/"

  max_steps: 50000

  # clip the gradient to have l2 norm at most this value
  gradient_clip_val: 10.0

  # whether to use automatic mixed precision
  use_amp: True
  # value to cast to in mixed precision training.
  precision: float16
  
  batch_size: 4 # number of examples placed on each GPU

  wandb_project: "o2nc_opt"
  wandb_logs_per_sec: 1.0
  
  # this will slow down computation a bit (I believe due to extra GPU/CPU communication),
  # but will log more stuff (like learning rates).
  # Still working on nice way to do this logging - we really should only incur one communication
  # round per iteration and I don't think the logging data should significantly impact it.
  log_callback_data: True
  
  running_stats_window: 1000

  # following settings chosen after
  # some experimentation with a tiny model.
  # may not be optimal for all machines, but
  # hopefully with a reasonably sized model this will
  # prevent dataloading from being the bottleneck.
  dataloader_workers: 2

optimizer:
  # learning rate scheduler
  lr: 1e-4
  lr_schedule: "cosine"
  lr_warmup: 5000

  # whether wraps the online learner with o2nc/eo2nc. values in ["o2nc", "eo2nc", null].
  wrap_o2nc: "o2nc"

  # PRNGKey seed for random scaling
  seed: 0

  # random scaling function. values in ["exponential", ...]
  random_scaling: "exponential"

  # regularization constants 
  # NOTE: exponentiation should be specified in online learner since each online learner
  # implements exponentiated gradient in its own way.
  # weight_decay: 0.01

  # base online learner. currently supports
  # ["unconstrained_ogd", "ada_ftrl", "blackbox_ftrl"]
  # Ada-FTRL
  # online_learner: "ada_ftrl"
  # ol_config:
  #   beta1: 0.9
  #   beta2: 0.999
  #   eps: 1e-8
  #   scale_exponential: False 
  #   scale_lr: False 
  #   scale_eps: False
  # Blackbox FTRL
  online_learner: "normalized_blackbox"
  ol_config:
    magnitude_learner: "kt"
    eps: 5
    Lipschitz: 20
    beta: 1.0
    weight_decay: 0.0


benchmark:
  # if true, runs adamw benchmark.
  run_benchmark: False

  # if true, uses optax.adamw. should always set to False.
  use_default: False

  lr: 3e-4
  lr_schedule: "cosine"
  lr_warmup: 5000

  beta1: 0.9
  beta2: 0.999
  weight_decay: 0.01
  debias_beta1: True
  debias_beta2: True